nnでは活性化関数の選択も重要
※活性化関数とはノードの階層間で使われる関数で複数の種類がある
実行する中で適したものを選択する

今回は、次の形のネットワークを定義することにします。

第1層
構造：LSTM
幅：100
活性化関数：tanh
第2層
構造：Dense
幅：20
活性化関数：relu
出力層
構造：Dense
幅：1
活性化関数：sigmoid
入力層となる第1層では、入力するデータの形を指定する必要があります。
引数 batch_input_shape に対して、batch_input_shape = (バッチサイズ, 説明変数の時系列の長さ, 説明変数の特徴量の種類) のようにX_train_t の形を指定します。
ただし、バッチサイズについては後ほど任意の大きさを指定できるように、今回は None としておきます。 
また、最終層となる出力層の活性化関数である sigmoid は0から1の値を出力します。この値を翌取引日の株価が上昇する確率とみなします。

ネットワークの定義が完了したらcompile関数によりモデルの確定を行います。
この時に、損失関数、最適化アルゴリズム、評価指標を指定することができます。
今回の予測では、株価が上がるか否かの2値のいずれかを当てるかという問題であるため、 
損失関数にはbinary_crossentropyを指定します。
また、 最適化アルゴリズムについては学習の収束速度に優れたAdamを用います。そして、評価指標はaccuracyを採用して計測します。

# Sequentialのインポート
from keras.models import Sequential
# Dense、LSTMのインポート
from keras.layers import Dense, LSTM

# ネットワークの各層のサイズの定義
num_l1 = 100
num_l2 = 20
num_output = 1

# 以下、ネットワークを構築
model = Sequential()
# 第1層
model.add(LSTM(units=num_l1,
                activation='tanh',
                batch_input_shape=(None, X_train_t.shape[1], X_train_t.shape[2])))
# 第2層
model.add(Dense(num_l2, activation='relu'))
# 出力層
model.add(Dense(num_output, activation='sigmoid'))
# ネットワークのコンパイル
model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])

モデルの学習
コンパイルしたモデルで学習を行うために、fit関数を適用します。
今回は、学習データ全体を何回学習させるかといった回数であるエポック数と、
1回のエポックの中で、細切れにしてモデルに与えるデータ数であるミニバッチサイズをそれぞれ次の通りに指定します。

エポック数：80
ミニバッチサイズ：24
さらに、学習データに対する過学習の程度をエポック毎に確認するために、
検証データによる予測精度の確認も行います。
このために、fit関数のオプションとして、validation_data=(X_val_t, y_val_t) を記載することで、
1回のエポックが終わるごとに、検証データを用いた予測精度の計算が行われます。
なお、fit関数の実行結果をresultとして格納します。resultには学習の経過が保存されており、
後ほど、モデルの学習の程度を可視化して確認する際に利用します。

以上の内容をまとめると、fit関数の引数としては次のように指定します。

result = model.fit(x=学習データの説明変数, y=学習データの目的変数, epochs=エポック数, batch_size=ミニバッチサイズ, 
validation_data=(検証データの説明変数, 検証データの目的変数)

# モデルの学習の実行（学習の完了までには数秒から数十秒ほど時間がかかります。）
result = model.fit(x=X_train_t, y=y_train_t, epochs=80, batch_size=24, validation_data=(X_val_t, y_val_t))
