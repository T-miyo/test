dropout法という方法がある
具体的には学習時のセルを最大50％を限度に設定する事でなだらかな学習になる事で過学習の程度を緩和し
予測の精度を高められる事がある

Dropoutの追加
KerasでDropoutを使う場合には、まずkeras.layersからDropoutをインポートする必要があります。

from keras.layers import Dropout

その上で、Sequential()を用いてネットワークを定義している場合は、Dropoutを適用したい層の追加の直後にDropoutを追加します。
たとえば、次のようなLSTMの層に対してDropoutを追加したい場合は

model.add(LSTM(units=層のサイズ, activation=活性化関数の種類))

まずは層をmodelに追加して、これに続けて

model.add(Dropout(Dropさせる割合))

と記述します。なお、Dropさせる割合については、0から1の間の値を入力します。
元々の層のサイズの半分以下をDropするように、0.5以下の値が指定されることが多いです。

# Dropoutのインポート
from keras.layers import Dropout

# ネットワークの各層のサイズの定義
num_l1 = 100
num_l2 = 20
num_output = 1

# Dropoutの割合の定義
dropout_rate = 0.4

# 以下、ネットワークを構築
model = Sequential()
# 第1層
model.add(LSTM(units=num_l1,
                activation='tanh',
                batch_input_shape=(None, X_train_t.shape[1], X_train_t.shape[2])))
model.add(Dropout(dropout_rate))
# 第2層
model.add(Dense(num_l2, activation='relu'))
model.add(Dropout(dropout_rate))
# 出力層
model.add(Dense(num_output, activation='sigmoid'))
# ネットワークのコンパイル
model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])

# モデルの学習の実行（学習の完了までには数秒から数十秒ほど時間がかかります。）
result = model.fit(x=X_train_t, y=y_train_t, epochs=80, batch_size=24, validation_data=(X_val_t, y_val_t))
