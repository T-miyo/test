※ラッパー法のコードを参考にメモ
ラッパー法（反復特徴量選択）は学習を行いながら重要な特徴量を選択する手法です。
ラッパー法（反復特徴量選択）には、次の二つの方法があります。
後退法：全く特徴量を使わないところからある基準が満たされるところまで1つずつ特徴量を加えていく方法
前進法：全ての特徴量を使う状態から1つずつ特徴量を取り除いていく方法
そしてラッパー法（反復特徴量選択）の一つが再帰的特徴量削減（recursive feature elimination：RFE）で、
RFEは全ての特徴量から開始してモデルを作成しそのモデルで最も重要度が低い特徴量を削減します。
そしてまたモデルを作って最も重要度が低い特徴量を削減します。この過程を事前に定めた数の特徴量になるまで繰り返します。
ラッパー法（反復特徴量選択）が機能するためには、モデルベース選択の場合と同様に、選択に用いるモデルが特徴量の重要性を決定する方法を提供していなければなりません。

import numpy as np
from sklearn.datasets import load_breast_cancer
from sklearn.model_selection import train_test_split
from sklearn.feature_selection import RFE
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression

#cancerデータセットを使用するためにロードする
cancer = load_breast_cancer()

#シードを指定して乱数を生成する
rng = np.random.RandomState(42)

#numpy.random.normal()を用いて正規分布に従う乱数を出力する
noise = rng.normal(size= (len(cancer.data), 50))

#フィルタ法によってノイズが取り除かれてモデルの性能が高まることを期待して、ノイズ特徴量をデータに加えておく
#最初の30特徴量はデータセットから来たもので、続く50の特徴量はノイズである。
X_w_noise = np.hstack([cancer.data, noise])

#trainデータとtestデータに分割する
X_train, X_test, y_train, y_test = train_test_split(X_w_noise, cancer.target, random_state=0, test_size=.5)

select = RFE(RandomForestClassifier(n_estimators=100, random_state=42), n_features_to_select=40)
select.fit(X_train, y_train)

X_train_rfe = select.transform(X_train)
X_test_rfe = select.transform(X_test)

#RFEを用いていない場合のスコア
score = LogisticRegression().fit(X_train, y_train).score(X_test, y_test)
print(f"X_train.shape : {X_train.shape}")
print(f'Test score : {score:.3f}')

print('-'*50)

#RFEを用いて選択した特徴量を用いた場合のスコア
score = LogisticRegression().fit(X_train_rfe, y_train).score(X_test_rfe, y_test)
print(f"X_train_rfe.shape : {X_train_rfe.shape}")
print(f'Score with only selected features : {score:.3f}')
