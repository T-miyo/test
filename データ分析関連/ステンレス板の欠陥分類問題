特徴量の選択がかなり大きく影響する為一番重視してみた
「#7つの特徴量を削除
drop_col=["X_Maximum","Y_Maximum","X_Perimeter","Sum_of_Luminosity","TypeOfSteel_A300","Y_Perimeter","LogOfAreas"]
train_df=train_df.drop(drop_col,axis=1)」

最終的に「Log_Y_Index」は除外して0.77から0.78に正答率が上がった
※外れ値がある為、外れ値の影響を下げる為カテゴリカル変数に変換
その後、ワンホットエンコーディングしてlightgblで処理出来る様に変換

「#カテゴリカル変数に変換
train_df['Pixels_Areas_qcut'] = pd.qcut(train_df['Pixels_Areas'], 4)
train_df['Log_Y_Index_qcut'] = pd.qcut(train_df['Log_Y_Index'], 4)」

「#One-Hot Encoding
train_df = pd.get_dummies(train_df, columns=['Log_Y_Index_qcut','Pixels_Areas_qcut'], dtype=int)」

「train_df = train_df.rename(columns={"Pixels_Areas_qcut_(5.999, 85.0]" : "Pixels_Areas_qcut_1", 
"Pixels_Areas_qcut_(85.0, 172.0]" : "Pixels_Areas_qcut_2", "Pixels_Areas_qcut_(172.0, 830.0]" : "Pixels_Areas_qcut_3", 
"Pixels_Areas_qcut_(830.0, 152655.0]" : "Pixels_Areas_qcut_4", "Log_Y_Index_qcut_(0.3, 1.079]" : "Log_Y_Index_qcut_1", 
"Log_Y_Index_qcut_(1.079, 1.322]" : "Log_Y_Index_qcut_2", 
"Log_Y_Index_qcut_(1.322, 1.74]" : "Log_Y_Index_qcut_3", "Log_Y_Index_qcut_(1.74, 4.259]" : "Log_Y_Index_qcut_4"})」
※カラムはトレーニングとテストで区切りの値が違う可能性があるので同じカラム名にして処理出来る様にする

※試しにアンサンブル学習のスタッキングを試したら0.01だが改善
lightgbm、ランダムフォレスト、エクストラツリーで組み合わせた
# バギングアンサンブル
estimators=[('rf', model_Ran), ('ex', ext_clf), ('lb', model_LGB)]
stk_clf=StackingClassifier(estimators=estimators, final_estimator=LogisticRegression())
stk_clf.fit(X_train, y_train)
results = model_selection.cross_val_score(stk_clf, X_val, y_val, cv = kfold) 
scores[('4.Stacking', 'train_score')] = results.mean()
scores[('4.Stacking', 'test_score')] = stk_clf.score(X_val, y_val)
※学習したら学習器と同じ様に
pred = stk_clf.predict(予測に使用するデータ)

scores
