モデル学習【学習用関数確認2】

# ライブラリのインポート
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset
from torchvision import datasets, models, transforms
from PIL import Image, ImageFilter

DEVICE= "cpu"

# モデル訓練用関数
def train_model(model, criterion, optimizer, num_epochs=5,is_saved = False):
    best_acc = 0.0

    # エポック数だけ下記工程の繰り返し
    for epoch in range(num_epochs):

        for phase in ['train', 'val']:
            print('{}:フェイズ'.format(phase))

            # 訓練フェイズと検証フェイズの切り替え
            if phase == 'train':
                model.train() 
            else:
                model.eval()  

            running_loss = 0.0
            running_corrects = 0

            # dataloadersからバッチサイズだけデータ取り出し、下記工程（1−5）の繰り返し
            for i,(inputs, labels) in enumerate(image_dataloaders[phase]):
                inputs = inputs.to(DEVICE)
                labels = labels.to(DEVICE)

                # 1. optimizerの勾配初期化
                optimizer.zero_grad()
                    
                # 2.モデルに入力データをinputし、outputを取り出す
                outputs = model(inputs)
                _, preds = torch.max(outputs, 1)

                # 3. outputと正解ラベルから、lossを算出
                loss = criterion(outputs, labels)
                print('   loaders:{}回目'.format(i+1)  ,'   loss:{}'.format(loss))

                if phase == 'train':                        
                    # 4. 誤差逆伝播法により勾配の算出
                    loss.backward()
                    # 5. optimizerのパラメータ更新
                    optimizer.step()

                running_loss += loss.item() * inputs.size(0)
                running_corrects += torch.sum(preds == labels.data)

            epoch_loss = running_loss / dataset_sizes[phase]
            epoch_acc = running_corrects.double() / dataset_sizes[phase]
            print('{} Loss: {:.4f} Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc))

            # C. 今までのエポックでの精度よりも高い場合はモデルの保存
            if phase == 'val' and epoch_acc > best_acc:
                best_acc = epoch_acc
                if(is_saved):
                    torch.save(model.state_dict(), './original_model_{}.pth'.format(epoch))

    print('Best val Acc: {:4f}'.format(best_acc))

# エポック数だけ下記工程の繰り返し
for epoch in range(num_epochs):
  
学習関数の最初の部分になります。
この部分でエポック回数分繰り返しの処理を行います。

for phase in ['train', 'val']:
    print('{}:フェイズ'.format(phase))

    # 訓練フェイズと検証フェイズの切り替え
    if phase == 'train':
        model.train() 
    else:
        model.eval()

学習時と検証時のフェイズを学習→検証の順番で切り替えます。
このプログラムはエポック数のfor文の中に記載されているので、
1エポック毎にtrainフェイズとvalフェイズ、2つのフェイズを動作させることになります。
またPyTorchではモデル学習時にフェイズ毎にモードを切り替える必要がある
そのため、.train()と.eval()を用いてモードの切り替えを行なっています。

# dataloadersからバッチサイズだけデータ取り出し、下記工程（1−5）の繰り返し
for i,(inputs, labels) in enumerate(image_dataloaders[phase]):
    inputs = inputs.to(DEVICE)
    labels = labels.to(DEVICE)
ここではdataloadersからバッチサイズだけデータ取り出し、インプットデータを取得しています。
この部分は、フェイズ切り替えのfor文の中に記載されているので、学習フェイズと検証フェイズのそれぞれに対して、
バッチサイズ/学習（検証）画像数の繰り返しを行います

入れ子が多くわかりにくいので、例を用いて考えていきましょう。
学習用データとして画像が120枚、検証用データとして80枚。バッチサイズは学習時も検証時も4とします。
その場合、訓練フェイズでは120/4=30回のdataloaderの繰り返しが起こり、検証フェイズでは80/4=20回のdataloaderの繰り返しが行われます。
このようにバッチサイズに応じて学習を行うことをミニバッチ学習と言います。
