評価データの予測
最後にあらためて、評価データの精度評価を行います。評価データに対する正解率については次のように計算できるのでした。

評価データの損失, 評価データの正解率 = model.evaluate(評価データの説明変数, 評価データの目的変数)

ただし、今回は、model.evaluateを用いて正解率を計算するのではなく、まずはモデルが算出した予測結果を確認して、
その後に、予測結果と評価データの目的変数との一致の程度を評価することにします。

モデルの予測結果はpredict関数を使って次のように取得することができます。

評価データの予測結果 = model.predict(評価データの説明変数)

取得された評価データの予測結果は、0から1の間をとる浮動小数点数となっており、
0.5を境にして、1に近いほど株価が上昇、0に近いほど株価が上昇せずと解釈することができます。

さて、評価データの目的変数は0もしくは1のどちらかのみをとる値となっていました。
正解率は、この評価データの目的変数との一致の程度として計算されるため、
評価データの予測結果も0もしくは1のどちらかのみをとる値に変換する必要があります。
そこで、Numpyのround関数を使って予測結果を四捨五入し、0もしくは1に丸めます。

丸め済の評価データの予測結果 = np.round(評価データの予測結果)

最後に、評価データの目的変数と丸め済の評価データの予測結果の一致の程度を計算するために、sklearnの accuracy_scoreを利用します。

評価データの正解率 = accuracy_score(y_true=評価データの目的変数, y_pred=丸め済の評価データの予測結果)


# ライブラリのインポート
import numpy as np
from sklearn.metrics import accuracy_score

# 評価データの予測結果の算出
pred_prob = model.predict(X_test_t)

# 予測結果の先頭10件を確認
print('予測結果の先頭10件')
print(pred_prob[:10])

# 評価データの予測結果を0もしくは1に丸め込み
pred = np.round(pred_prob)

# 丸め込んだ予測結果の先頭10件を確認
print('丸め込んだ予測結果の先頭10件')
print(pred[:10])

# 評価データの正解率の計算
accuracy = accuracy_score(y_true=y_test_t, y_pred=pred)

# 評価データの正解率の表示
print('評価データの正解率:', accuracy)
