LightGBM Tuner は LightGBM に特化したハイパーパラメータ自動最適化のためのモジュール「Optuna」を使用した際に
スムーズにいかなかった為メモ
※参照元：https://qiita.com/askl4267/items/28b476f76b01699430fe
kaggleなどブラウザでコーディングする場合のpipは「!pip install optuna optuna-integration」
ローカル環境のVscodeで上記のコマンドを実施した際エラーになった為「pip install optuna optuna-integration--user」
で実施してエラーを回避。警告が出るがコードには影響ないので今回はこれで良しとする

Optuna を使って LightGBM のハイパーパラメータをチューニングするとき、ナイーブに実装すると次のようになります。

def objective(trial):
    data, target = sklearn.datasets.load_breast_cancer(return_X_y=True)
    train_x, test_x, train_y, test_y = train_test_split(data, target, test_size=0.25)
    dtrain = lgb.Dataset(train_x, label=train_y)
 
    param = {
        'objective': 'binary',
        'metric': 'binary_logloss',
        'lambda_l1': trial.suggest_loguniform('lambda_l1', 1e-8, 10.0),
        'lambda_l2': trial.suggest_loguniform('lambda_l2', 1e-8, 10.0),
        'num_leaves': trial.suggest_int('num_leaves', 2, 256),
        'feature_fraction': trial.suggest_uniform('feature_fraction', 0.4, 1.0),
        'bagging_fraction': trial.suggest_uniform('bagging_fraction', 0.4, 1.0),
        'bagging_freq': trial.suggest_int('bagging_freq', 1, 7),
        'min_child_samples': trial.suggest_int('min_child_samples', 5, 100),
    }
 
    gbm = lgb.train(param, dtrain)
    preds = gbm.predict(test_x)
    pred_labels = np.rint(preds)
    accuracy = sklearn.metrics.accuracy_score(test_y, pred_labels)
    return accuracy
 
study = optuna.create_study(direction='maximize')
study.optimize(objective, n_trials=100)
 
print('Number of finished trials:', len(study.trials))
print('Best trial:', study.best_trial.params)

上記のコードをPyhton コードの import 文を 1 行変更するだけで簡単に利用できます。

ナイーブな LightGBM のハイパーパラメータチューニング
LightGBM は勾配ブースティング法の高速な実装を提供する人気のライブラリです。
様々な機能が実装されており便利である反面、多くのハイパーパラメータが用意されています。
例えば、決定木の葉の数や深さを制御するハイパーパラメータ、決定木ごとにサンプリングする特徴量の割合、
決定木の葉に割り当てられる最小のサンプル数などなど。これらを複数回の試行なしにチューニングすることは容易ではありません。
Optuna はこうしたハイパーパラメータのチューニングを手助けするフレームワークです。
過去の試行結果に基づいて、効率的に最適なハイパーパラメータを探索する手助けをします。
