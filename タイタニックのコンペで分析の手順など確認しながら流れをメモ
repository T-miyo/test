データの前処理が重要なのは具体的に掴めたので
まず、データの確認
各カラムの簡単な説明をは以下の通り。

・PassengerId – 乗客識別ユニークID
・Survived – 生存フラグ（0=死亡、1=生存）
・Pclass – チケットクラス
・Name – 乗客の名前
・Sex – 性別（male=男性、female＝女性）
・Age – 年齢
・SibSp – タイタニックに同乗している兄弟/配偶者の数
・parch – タイタニックに同乗している親/子供の数
・ticket – チケット番号
・fare – 料金
・cabin – 客室番号
・Embarked – 出港地（タイタニックへ乗った港）

さらに各変数の簡単な説明も記載をしておきます。
pclass = チケットクラス
1 = 上層クラス（お金持ち）
2 = 中級クラス（一般階級）
3 = 下層クラス（労働階級）

Embarked = 各変数の定義は下記の通り
C = Cherbourg
Q = Queenstown
S = Southampton
NaNはデータの欠損を表します。
・提供されるトレーニングデータとテストデータをまとめて欠損値を確認し、項目の性質などに応じて中央値などで置換する
・トレーニングデータの欠損など確認して
　分析に必要な形に変換するか分析対象から除外するかを判断する
　Age、Cabin、Embarkedに欠損値があることがわかりました。
　一番欠損が多い「Cabinは客室番号」を詳しく見る
　value_counts()でCabinカラムを推測や確からしい値を入れることができないか判断する
　Cabinカラムには、147種類の行がある為、確からしい値入れることも難しいという判断で分析には除外と判断

　Ageの欠損値の対応をします。
　Ageは、欠損値が比較的多く (計263箇所) 、
  平均値や中央値で補完してしまうと、元のヒストグラムのバランスを崩してしまうことになる。
  そのため、機械学習のランダムフォレストを用いて補完を行なっていく。
※Ageの補完に適するパラメータを選定する
　数字化できていない項目を数字化する
　ランダムフォレストを用いてAgeを予測し、保管する
　trainデータとtestデータを組み合わせる
　組み合わせたデータからAgeが欠損している行を削除
　作ったデータを元にランダムフォレストのモデルを構築
　上記のモデルを使って、Ageを補完を行う
　モデルの確からしさを判断するため、欠損値補完前後のヒストグラムを比較する
　現在与えられている情報からAge (年齢) を予測するパラメータとして、下記の項目を選定
　パラメータ：Pclass, Sex, SibSp, Parch, Fare
　＊乗船港は年齢に影響を与えないと考え除外
　　Sexはカテゴリーデータ (female, male) の文字列で与えられているので、数字化を行う
　＊数字化においては、最終予測の機械学習時に変換が必要になるため、元データを変換する
具体的なコード
# map関数を用いてSexの文字をそれぞれの数字に変換する
datasets = [df_train, df_test]
for df in datasets:
    df['Sex'] = df['Sex'].map( {'female': 0, 'male': 1} ).astype(int)
機械学習(ランダムフォレスト)を用いて欠損値のAgeを予測する
以下がコード
# ランダムフォレストをインポートする
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report

# パラメータをピックアップし、リストに代入しておく
features = ["Pclass", "Sex", "SibSp", "Parch", "Fare"]

# trainデータとtestデータを縦に連結
df_concat = pd.concat([df_train, df_test], ignore_index=True)

# 欠損値のない行をトレーニングデータとして使用
train_data = df_concat.dropna(subset=['Age'])

# データの前処理
X_train = train_data[features]
y_train = train_data['Age']

# ランダムフォレスト回帰モデルのトレーニング
rf_model = RandomForestRegressor(random_state=42)
rf_model.fit(X_train, y_train)

# モデルを用いてtrainデータのAgeを予測し補完
df_train.loc[df_train['Age'].isnull(), 'Age'] = rf_model.predict(df_train.loc[df_train['Age'].isnull(), features] )

# Testデータも同様に予測値を適用する
df_test.loc[df_test['Age'].isnull(), 'Age'] = rf_model.predict(df_test.loc[df_test['Age'].isnull(), features] )

# 元のヒストグラムと傾向性が違っていないかを確認する
df_concat_after = pd.concat([df_train, df_test], ignore_index=True)

fig, ax = plt.subplots(1,2,figsize=(12,5), sharey=True)
sns.histplot(data=df_concat, x='Age', bins=30, color='gray', stat='percent', ax=ax[0])
sns.histplot(data=df_concat_after, x='Age', bins=30, stat='percent', ax=ax[1])
plt.show()

・Embarkedは最頻値で補完
・Fareは、中央値で補完を行うことを考える。ただし、FareとPclassで関係性が高いと思うため、Pclassに応じた中央値での補完
ex)# PclassごとのFareの中央値を計算したpivot_tableの作成
   median_Fare = df_train.pivot_table(index='Pclass', values='Fare', aggfunc='median')
   median_Fare
・データの確認と理解により「Name」「Ticket」「Cabin」は学習データには使わない
・
・機械に学習をさせていきます。
　RandomForestClassifierのn_estimatorsとmax_depthを変えるなかで最も良い予測モデルを作成します。
